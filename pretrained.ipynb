{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsG6SG9ys_Ee",
        "outputId": "57925d4f-2ec5-4d8b-c005-8447cd059313"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence-transformers-2.1.0.tar.gz (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /Users/divyansh/opt/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (4.13.0)\n",
            "Requirement already satisfied: tokenizers>=0.10.3 in /Users/divyansh/opt/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (0.10.3)\n",
            "Requirement already satisfied: tqdm in /Users/divyansh/opt/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (4.62.3)\n",
            "Requirement already satisfied: torch>=1.6.0 in /Users/divyansh/opt/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (1.10.0)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.11.1-cp38-cp38-macosx_10_9_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /Users/divyansh/opt/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (1.20.1)\n",
            "Requirement already satisfied: scikit-learn in /Users/divyansh/opt/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (0.24.1)\n",
            "Requirement already satisfied: scipy in /Users/divyansh/opt/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (1.6.2)\n",
            "Requirement already satisfied: nltk in /Users/divyansh/opt/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (3.6.1)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp38-cp38-macosx_10_6_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 13.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub in /Users/divyansh/opt/anaconda3/lib/python3.8/site-packages (from sentence-transformers) (0.2.1)\n",
            "Requirement already satisfied: typing-extensions in /Users/divyansh/opt/anaconda3/lib/python3.8/site-packages (from torch>=1.6.0->sentence-transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/divyansh/opt/anaconda3/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (5.4.1)\n",
            "Requirement already satisfied: requests in /Users/divyansh/opt/anaconda3/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/divyansh/opt/anaconda3/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/divyansh/opt/anaconda3/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.4.4)\n",
            "Requirement already satisfied: sacremoses in /Users/divyansh/opt/anaconda3/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.46)\n",
            "Requirement already satisfied: filelock in /Users/divyansh/opt/anaconda3/lib/python3.8/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /Users/divyansh/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied: joblib in /Users/divyansh/opt/anaconda3/lib/python3.8/site-packages (from nltk->sentence-transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /Users/divyansh/opt/anaconda3/lib/python3.8/site-packages (from nltk->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/divyansh/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /Users/divyansh/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/divyansh/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/divyansh/opt/anaconda3/lib/python3.8/site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (4.0.0)\n",
            "Requirement already satisfied: six in /Users/divyansh/opt/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/divyansh/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->sentence-transformers) (2.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /Users/divyansh/opt/anaconda3/lib/python3.8/site-packages (from torchvision->sentence-transformers) (8.2.0)\n",
            "Building wheels for collected packages: sentence-transformers\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.1.0-py3-none-any.whl size=120999 sha256=b328303735c777bddc74f63b8cbd25f0abbf6eb40d491b5b2f0d8920a2dc0bf5\n",
            "  Stored in directory: /Users/divyansh/Library/Caches/pip/wheels/52/19/88/6625593382e23a926740e6fcee0f2df0a0de25766094842a28\n",
            "Successfully built sentence-transformers\n",
            "Installing collected packages: torchvision, sentencepiece, sentence-transformers\n",
            "Successfully installed sentence-transformers-2.1.0 sentencepiece-0.1.96 torchvision-0.11.1\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2mO16ZSFcrIn"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "def calculateSimilarities(sentences, model = 'nreimers/TinyBERT_L-6_H-768_v2', tokeniser = 'nreimers/TinyBERT_L-6_H-768_v2'):\n",
        "\n",
        "  \"\"\"\n",
        "    Calculate the similarity between each sentence in the list of sentences using the BERT model.\n",
        "    The sentences are tokenised using the tokeniser.\n",
        "    The model is the BERT model to use.\n",
        "    The tokeniser is the tokeniser to use.\n",
        "    The sentences are tokenised using the tokeniser.\n",
        "\n",
        "    :param sentences: The list of sentences to calculate the similarity between first and rest of all.\n",
        "    :param model: The BERT model to use.\n",
        "    :param tokeniser: The tokeniser to use.\n",
        "    :return: A list containing the similarity between the first sentence and the rest of the sentences.\n",
        "\n",
        "    Example:\n",
        "    sentences = ['I like to eat', 'I like to eat too', 'I like to eat too much']\n",
        "    calculateSimilarities(sentences)\n",
        "    [0.9, 0.8]\n",
        "\n",
        "    Example:\n",
        "    sentences = ['I like to eat', 'I like to eat too', 'I like to eat too much']\n",
        "    calculateSimilarities(sentences, model = 'nreimers/TinyBERT_L-6_H-768_v2', tokeniser = 'nreimers/TinyBERT_L-6_H-768_v2')\n",
        "    [0.9, 0.8]\n",
        "  \"\"\"\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\n",
        "  model = AutoModel.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\n",
        "  \n",
        "  # initialize dictionary to store tokenized sentences\n",
        "  tokens = {'input_ids': [], 'attention_mask': []}\n",
        "\n",
        "  for sentence in sentences:\n",
        "    # encode each sentence and append to dictionary\n",
        "    new_tokens = tokenizer.encode_plus(sentence, max_length=128,\n",
        "                                       truncation=True, padding='max_length',\n",
        "                                       return_tensors='pt')\n",
        "    tokens['input_ids'].append(new_tokens['input_ids'][0])\n",
        "    tokens['attention_mask'].append(new_tokens['attention_mask'][0])\n",
        "\n",
        "  \n",
        "  # reformat list of tensors into single tensor\n",
        "  tokens['input_ids'] = torch.stack(tokens['input_ids'])\n",
        "  tokens['attention_mask'] = torch.stack(tokens['attention_mask'])\n",
        "\n",
        "  # We process these tokens through our model:\n",
        "  outputs = model(**tokens)\n",
        "\n",
        "  # The dense vector representations of our text are contained within the outputs \n",
        "  # 'last_hidden_state' tensor, which we access like so:\n",
        "\n",
        "  embeddings = outputs.last_hidden_state\n",
        "\n",
        "\n",
        "  # To perform this operation, we first resize our attention_mask tensor:\n",
        "  attention_mask = tokens['attention_mask']\n",
        "  \n",
        "  mask = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
        "\n",
        "  # Each vector above represents a single token attention mask - each token now has a vector of size 768 representing it's attention_mask status. Then we multiply the two tensors to apply the attention mask:\n",
        "\n",
        "  masked_embeddings = embeddings * mask\n",
        "\n",
        "  # Then we sum the remained of the embeddings along axis 1:\n",
        "  summed = torch.sum(masked_embeddings, 1)\n",
        "\n",
        "  # Then sum the number of values that must be given attention in each position \n",
        "  # of the tensor:\n",
        "  summed_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
        "\n",
        "  # Finally, we calculate the mean as the sum of the embedding activations summed \n",
        "  # divided by the number of values that should be given attention in each\n",
        "  # position summed_mask:\n",
        "\n",
        "  mean_pooled = summed / summed_mask\n",
        "\n",
        "  return cosineSimilarity(mean_pooled)[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-u6hSluvgU4O"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def cosineSimilarity(mean_pooled):\n",
        "  \"\"\"\n",
        "    Calculate the cosine similarity between the mean pooled embeddings of the sentences.\n",
        "\n",
        "    :param mean_pooled: The mean pooled embeddings of the sentences.\n",
        "    :return: The cosine similarity between the mean pooled embeddings of the sentences.\n",
        "  \"\"\"\n",
        "  mean_pooled = mean_pooled.detach().numpy()\n",
        "\n",
        "  # calculate\n",
        "  return cosine_similarity(\n",
        "      [mean_pooled[0]],\n",
        "      mean_pooled[1:]\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8QJLqX4SiJHx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting prettytable\n",
            "  Downloading prettytable-2.4.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: wcwidth in /Users/divyansh/opt/anaconda3/lib/python3.8/site-packages (from prettytable) (0.2.5)\n",
            "Installing collected packages: prettytable\n",
            "Successfully installed prettytable-2.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install prettytable\n",
        "from prettytable import PrettyTable\n",
        "def findSimilarities(theSentence, sentences, model, tokeniser):\n",
        "  \"\"\"\n",
        "    Find the similarity between the given sentence and the rest of the sentences.\n",
        "\n",
        "    :param theSentence: The sentence to find the similarity of.\n",
        "    :param sentences: The list of sentences to find the similarity with.\n",
        "    :param model: The BERT model to use.\n",
        "    :param tokeniser: The tokeniser to use.\n",
        "    :prints: A list containing the similarity between the given sentence and the rest of the sentences.\n",
        "\n",
        "    Example:\n",
        "    sentences = ['I like to eat', 'I like to eat too', 'I like to eat too much']\n",
        "    findSimilarities('I like to eat', sentences)\n",
        "    Output: \n",
        "          Sentence :  I like to eat.\n",
        "      NUmber of sentences to compare to :  3\n",
        "      +-------+------------------------+------------+\n",
        "      | Index |        Sentence        | Similarity |\n",
        "      +-------+------------------------+------------+\n",
        "      |   1   |     I like to eat      | 0.99466836 |\n",
        "      |   2   |   I like to eat too    | 0.97190154 |\n",
        "      |   3   | I like to eat too much | 0.8314854  |\n",
        "      +-------+------------------------+------------+\n",
        "  \"\"\"\n",
        "  allSentences = [theSentence] + sentences\n",
        "  similarities = calculateSimilarities(allSentences)\n",
        "\n",
        "  print(\"Sentence : \" ,  theSentence)\n",
        "  print(\"Number of sentences to compare to : \", len(similarities))\n",
        "  myTable = PrettyTable([\"Index\", \"Sentence\", \"Similarity %\"])\n",
        "\n",
        "  for i in range(len(similarities)):\n",
        "    myTable.add_row([i+1, sentences[i], similarities[i]*100]) \n",
        "\n",
        "  print(myTable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZ_ZIXhMjjX4",
        "outputId": "b09124b7-c885-455b-85c1-e59f4029fa27"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: 100%|██████████| 399/399 [00:00<00:00, 154kB/s]\n",
            "Downloading: 100%|██████████| 625/625 [00:00<00:00, 446kB/s]\n",
            "Downloading: 100%|██████████| 226k/226k [00:01<00:00, 188kB/s]\n",
            "Downloading: 100%|██████████| 455k/455k [00:02<00:00, 159kB/s]\n",
            "Downloading: 100%|██████████| 2.00/2.00 [00:00<00:00, 690B/s]\n",
            "Downloading: 100%|██████████| 112/112 [00:00<00:00, 111kB/s]\n",
            "Downloading: 100%|██████████| 418M/418M [00:15<00:00, 28.7MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence :  I like to eat\n",
            "Number of sentences to compare to :  3\n",
            "+-------+------------------------+-------------------+\n",
            "| Index |        Sentence        |    Similarity %   |\n",
            "+-------+------------------------+-------------------+\n",
            "|   1   |     I like to eat      |       100.0       |\n",
            "|   2   |   I like to eat too    | 97.61621952056885 |\n",
            "|   3   | I like to eat too much | 83.08659195899963 |\n",
            "+-------+------------------------+-------------------+\n"
          ]
        }
      ],
      "source": [
        "sentences = ['I like to eat', 'I like to eat too', 'I like to eat too much']\n",
        "\n",
        "findSimilarities(\"I like to eat\", sentences, 'nreimers/TinyBERT_L-6_H-768_v2', 'nreimers/TinyBERT_L-6_H-768_v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsXzutfvs54r",
        "outputId": "07455659-6eed-4308-a511-6fca81d20566"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence :  He didnt like walmart, but dont shop anywhere.\n",
            "Number of sentences to compare to :  6\n",
            "+-------+---------------------------------------------------------------------------------------------------------+--------------------+\n",
            "| Index |                                                 Sentence                                                |    Similarity %    |\n",
            "+-------+---------------------------------------------------------------------------------------------------------+--------------------+\n",
            "|   1   | He was very excited when they officially decided to rename 'Columbus Day' as 'Indigenous People's Day'. | 11.694115400314331 |\n",
            "|   2   |          She didn't like to support Amazon, but she was too lazy to go shopping anywhere else.          | 78.61791849136353  |\n",
            "|   3   |                        She learned the different types of clouds in second grade.                       | 17.35464036464691  |\n",
            "|   4   |                   He was so surprised that he dropped the dumbbells right on his foot.                  | 46.022409200668335 |\n",
            "|   5   |                             Any cop who claims not to like donuts is a liar.                            | 61.31804585456848  |\n",
            "|   6   |                       What was I supposed to do, let her throw up on my begonias?                       | 29.20287549495697  |\n",
            "+-------+---------------------------------------------------------------------------------------------------------+--------------------+\n"
          ]
        }
      ],
      "source": [
        "sentences = [\n",
        "\"He was very excited when they officially decided to rename 'Columbus Day' as 'Indigenous People's Day'.\",\n",
        "\"She didn't like to support Amazon, but she was too lazy to go shopping anywhere else.\",\n",
        "\"She learned the different types of clouds in second grade.\",\n",
        "\"He was so surprised that he dropped the dumbbells right on his foot.\",\n",
        "\"Any cop who claims not to like donuts is a liar.\",\n",
        "\"What was I supposed to do, let her throw up on my begonias?\"\n",
        "]\n",
        "\n",
        "findSimilarities(\"He didnt like walmart, but dont shop anywhere.\", sentences, 'nreimers/TinyBERT_L-6_H-768_v2', 'nreimers/TinyBERT_L-6_H-768_v2')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "CWE.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
